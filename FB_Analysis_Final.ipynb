{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.collocations import *\n",
    "import nltk.draw\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import glob\n",
    "from nltk.corpus import stopwords\n",
    "from datetime import datetime\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#for ipython\n",
    "%matplotlib inline\n",
    "\n",
    "#pre-processing tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df= pd.read_csv('Fintech/Retail_Banks/MandSBank_facebook_statuses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df= pd.read_csv('DSI/CGI_FB/Fintech/Retail_Banks/')\n",
    "allFiles = glob.glob(\"Fintech/Retail_Banks/*.csv\")\n",
    "#allFiles = glob.glob(\"Uk_J*.csv\")\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df0 = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(df0)\n",
    "df = pd.concat(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df.status_published)\n",
    "df['month_yr'] = df.date.map(lambda x: x.strftime('%Y-%m'))\n",
    "df.groupby(['month_yr']).agg(['count']).status_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('Canada_Jan to June_2015.csv')\n",
    "df2 = pd.read_csv('Uk_Jan to June_2015.csv')\n",
    "df3 = pd.read_csv('Us_Jan to June_2015.csv')\n",
    "#df4 = pd.read_csv('Us_July to Dec_2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df12 = df1.append(df2, ignore_index=True)\n",
    "#df34 = df3.append(df4, ignore_index=True)\n",
    "#df   = df12.append(df34, ignore_index=True)\n",
    "df = df12.append(df3, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv('Canada_Jan to June_2014.csv')\n",
    "df = pd.read_csv('Uk_July to Dec_2014.csv')\n",
    "#df = pd.read_csv('Canada_Jan to June_2015.csv')\n",
    "#df = pd.read_csv('Canada_July to Dec_2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.status_published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df.status_published.map(lambda x: datetime.strptime(x,'%d-%m-%Y %H:%M')))\n",
    "df.date > datetime(2014, 12, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df0 = df[df.status_message.str.contains(\"http\") == False]\n",
    "#df1 = df0[df0.status_message.str.contains(\".com\") == False]\n",
    "\n",
    "tweets = list(df0.status_message.drop_duplicates())\n",
    "\n",
    "#tweet_processor = TweetPreprocessor()\n",
    "#tknzr = TweetTokenizer()\n",
    "#nltk.download('stopwords')\n",
    "#stop = stopwords.words('english')\n",
    "#stop += ['<hashtag>', '<url>', '<allcaps>', '<number>', '<user>', '<repeat>', '<elong>', 'websummit', 'battery', 'song']\n",
    "#outtweets = []\n",
    "#for tweet in tweets:\n",
    "#    parts = tknzr.tokenize(tweet_processor.preprocess(tweet))\n",
    "#    clean = [i.encode('ascii', 'ignore') for i in parts if i not in stop]\n",
    "#    outtweets.append(clean)\n",
    "tknzr = TweetTokenizer()\n",
    "tokens = tknzr.tokenize(str(tweets).translate(None, string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop += ['rt','im', 'thats', '.' , \\\n",
    "        ',', '?', '!']\n",
    "#stop += ['barclays', 'hsbc', 'lloyds', 'halifax', 'natwest', 'metro', 'wells', 'bank', 'account']\n",
    "#stop += ['wells', 'fargo', 'wellsfargo', 'pnc', 'suntrust', 'pnc', 'Ã§apital one']\n",
    "tweet_texts_processed = [i.lower().encode('ascii', 'ignore') for i in tokens if i.lower() not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = nltk.pos_tag(tweet_texts_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nouns = [word for word,pos in tags if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')]\n",
    "#downcased = [x.lower() for x in nouns]\n",
    "#joined = \" \".join(downcased).encode('ascii', 'ignore')\n",
    "#into_string = str(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_nouns=nltk.FreqDist(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_nouns.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('Canada_nouns.csv', 'w') as fp:\n",
    "    a = csv.writer(fp, delimiter=',')\n",
    "    data = freq_nouns.items()\n",
    "    a.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_nouns.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq_nouns.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textNLTK=nltk.Text(tweet_texts_processed)\n",
    "textNLTK.similar(\"gaza\") #Find similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuation = re.compile(r'[-.?!,\":;()|0-9]')\n",
    "tweet_texts_processed = [punctuation.sub(\"\", word) for word in tweet_texts_processed]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textNLTK=nltk.Text(tweet_texts_processed)\n",
    "textNLTK.similar(\"wearable\") #Find similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "                                       #####Rough work#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create your bigrams\n",
    "bgs = nltk.bigrams(tweet_texts_processed)\n",
    "tgs = nltk.trigrams(tweet_texts_processed)\n",
    "#compute frequency distribution for all the bigrams in the text\n",
    "fdist_bgs = nltk.FreqDist(bgs)\n",
    "fdist_tgs = nltk.FreqDist(tgs)\n",
    "#for k,v in fdist.items():\n",
    "#    print k,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fdist_bgs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('Canada_bigrams.csv', 'w') as fp:\n",
    "    a = csv.writer(fp, delimiter=',')\n",
    "    data = fdist_bgs.items()\n",
    "    a.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('Canada_trigrams.csv', 'w') as fp:\n",
    "    a = csv.writer(fp, delimiter=',')\n",
    "    data = fdist_tgs.items()\n",
    "    a.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textNLTK=nltk.Text(tweet_texts_processed)\n",
    "#textNLTK.collocations(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(fdist_bgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist_bgs.most_common(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist_tgs.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outtweets=nltk.Text(tokens) #convert to NLTK class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "tweet_texts_processed = [i.encode('ascii', 'ignore') for i in outtweets if i not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#removing numbers and punctuation using regexp\n",
    "\n",
    "punctuation = re.compile(r'[-.?!,\":;()|0-9]')\n",
    "tweet_texts_processed = [punctuation.sub(\"\", word) for word in tweet_texts_processed]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textNLTK=nltk.Text(tweet_texts_processed)\n",
    "textNLTK.similar(\"reward\") #Find similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist1 = nltk.FreqDist(textNLTK)\n",
    "print(dist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist1.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist1.plot(50, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "finderBi = BigramCollocationFinder.from_words(tweet_texts_processed)\n",
    "finderTri = TrigramCollocationFinder.from_words(tweet_texts_processed)\n",
    "\n",
    "finderBi.nbest(bigram_measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finderBi.apply_freq_filter(2) #filter out based on total occurrence of the word\n",
    "finderTri.apply_freq_filter(2) #filter out based on total occurrence of the word\n",
    "\n",
    "finderBi.apply_word_filter(lambda w: w in ('I', 'me', '(', ')', '/', ':', '*', '\"', 're', '?', ';', '-', '$'))\n",
    "finderTri.apply_word_filter(lambda w: w in ('I', 'me', '(', ')', '/', ':', '*', '\"', 're', '?', ';', '-', '$'))\n",
    "\n",
    "scoredBi = finderBi.score_ngrams(bigram_measures.raw_freq)\n",
    "scoredTri = finderBi.score_ngrams(trigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(bigram for bigram, score in scoredBi) == set(nltk.bigrams(tweet_texts_processed))\n",
    "sorted(finderBi.nbest(bigram_measures.raw_freq, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(trigram for trigram, score in scoredTri) == set(nltk.trigrams(tweets))\n",
    "sorted(finderTri.nbest(trigram_measures.raw_freq, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#POS tagging\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "sentence = \"Michael Jackson likes to eat at McDonalds\"\n",
    "tagged_sent = pos_tag(tweet_texts_processed)\n",
    "#(sentence.split())\n",
    "# [('Michael', 'NNP'), ('Jackson', 'NNP'), ('likes', 'VBZ'), ('to', 'TO'), ('eat', 'VB'), ('at', 'IN'), ('McDonalds', 'NNP')]\n",
    "\n",
    "propernouns = [word for word,pos in tagged_sent if pos == 'NNP']\n",
    "# ['Michael','Jackson', 'McDonalds']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
